# -*- coding: utf-8 -*-
"""ece046211_hw2_mlp_cnn_students.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJqv38EV0ftw76Q_E-6rLitkgCCH-msj

# <img src="https://img.icons8.com/bubbles/50/000000/mind-map.png" style="height:50px;display:inline"> ECE 046211 - Technion - Deep Learning
---

## HW2 - Multilayer NNs and Convolutional NNs
---

### <img src="https://img.icons8.com/clouds/96/000000/keyboard.png" style="height:50px;display:inline"> Keyboard Shortcuts
---
* Run current cell: **Ctrl + Enter**
* Run current cell and move to the next: **Shift + Enter**
* Show lines in a code cell: **Esc + L**
* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`
* New cell below: **Esc + B**
* Delete cell: **Esc + D, D** (two D's)

### <img src="https://img.icons8.com/bubbles/50/000000/information.png" style="height:50px;display:inline"> Students Information
---
* Fill in

|Name     |Campus Email| ID  |
|---------|--------------------------------|----------|
|Daniel Ochana| danielochana@campus.technion.ac.il| 318531126|
|Uri Koren| uri.koren@campus.technion.ac.il| 318174703|

### <img src="https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png" style="height:50px;display:inline"> Submission Guidelines
---
* Maximal garde: 100.
* Submission only in **pairs**.
    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).
* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.
* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>
* What you have to submit:
    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw2_id1_id2.ipynb`.
    * If you answered the questions in a different file you should submit a `.zip` file with the name `ece046211_hw2_id1_id2.zip` with content:
        * `ece046211_hw2_id1_id2.ipynb` - the code tasks
        * `ece046211_hw2_id1_id2.pdf` - answers to questions.
    * No other file-types (`.py`, `.docx`...) will be accepted.
* Submission on the course website (Moodle).
* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers ("* some text here with Latex equations" -> "some text here with Latex equations").

### <img src="https://img.icons8.com/dusk/64/000000/online.png" style="height:50px;display:inline"> Working Online and Locally
---
* You can choose your working environment:
    1. `Jupyter Notebook`, **locally** with <a href="https://www.anaconda.com/distribution/">Anaconda</a> or **online** on <a href="https://colab.research.google.com/">Google Colab</a>
        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\rightarrow$ `Change Runtime Type` $\rightarrow$`GPU`.
    2. Python IDE such as <a href="https://www.jetbrains.com/pycharm/">PyCharm</a> or <a href="https://code.visualstudio.com/">Visual Studio Code</a>.
        * Both allow editing and running Jupyter Notebooks.

* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.
* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images).

### <img src="https://img.icons8.com/bubbles/50/000000/checklist.png" style="height:50px;display:inline"> Agenda
---

* [Part 1 - Theory](#-Part-1---Theory)
    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)
    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)
    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)
    * [Q4 - Initialization](#-Question-4---Initialization)
    * [Q5 - Equivariance](#-Question-5---Equivariance)
    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)
* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)
    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)
    * [Task 2 - MLP-based Deep Classifer](#-Task-2---MLP-based-Deep-Classifer)
    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)
* [Credits](#-Credits)

### <img src="https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png" style="height:50px;display:inline"> Part 1 - Theory
---
* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.
* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.

* $\large\LaTeX$ <a href="https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index">Cheat-Sheet</a> (to write equations)
    * <a href="http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf">Another Cheat-Sheet</a>

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 1 -Generalization in A Teacher-Student Setup
---

Recall from lecture 4 the Bayes Risk $\mathcal{\overline{R}}(w)$: $$ \mathcal{\overline{R}}(w) \triangleq \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma_{\epsilon}^2I), w_{true} \sim \mathcal{N}(0,\frac{\sigma_w^2}{d}I) } \left[\mathcal{R}\right], $$ where, $$ \mathcal{R}(w_{\mu}) = ||w_{\mu}-w_{true}||^2 = ||(H_{\mu}^{-1}H-I)w_{true} + H_{\mu}^{-1}X^T\epsilon||^2 $$

Prove:

$$ \overline{\mathcal{R}}(w_{\mu}) = \sum_{i=1}^d \frac{(\sigma_w^2/d) \mu^2 + \sigma_{\epsilon}^2 \lambda_i}{(\lambda_i + \mu)^2} $$

Hints:
* $\mathbb{E} \left[\epsilon^TXH_{\mu}^{-1}H_{\mu}^{-1}X^T\epsilon \right] = \sum_{i,j}^N\mathbb{E}[\epsilon_i \epsilon_j] \left(XH_{\mu}^{-1} \right)_i\left(H_{\mu}^{-1}X^T \right)_j$

* $\mathbb{E}[\epsilon_i \epsilon_j] = \sigma_{\epsilon}^2 \delta_{ij}$

* $\sum_{i=1}^N \left(XH_{\mu}^{-1} \right)_i\left(H_{\mu}^{-1}X^T \right)_i = Tr\left[XH_{\mu}^{-2}X^T \right] $

**Answer:**

We observe that:
$$
||(H_{\mu}^{-1}H-I)w_{true} + H_{\mu}^{-1}X^T\epsilon||^2 = \left((H_{\mu}^{-1}H-I)w_{true} + H_{\mu}^{-1}X^T\epsilon \right)^T\left((H_{\mu}^{-1}H-I)w_{true} + H_{\mu}^{-1}X^T\epsilon \right)
\\ = \epsilon^TXH_{\mu}^{-1}H_{\mu}^{-1}X^T\epsilon
+ \epsilon^TXH_{\mu}^{-1}(H_{\mu}^{-1}H-I)w_{true}
+w_{true}^T(H_{\mu}^{-1}H-I)^T H_{\mu}^{-1}X^T\epsilon
+ w_{true}^T(H_{\mu}^{-1}H-I)^T (H_{\mu}^{-1}H-I)w_{true}
$$
Since the expected value is a linear function, we can analyze each of the terms in the sum independently. Observe the second term in the sum:
$$
\mathbb{E}[\epsilon^TXH_{\mu}^{-1}(H_{\mu}^{-1}H-I)w_{true}] \underset{A' = XH_{\mu}^{-1}(H_{\mu}^{-1}H-I)}{=} \mathbb{E}[\epsilon^T A' w_{true}] = \mathbb{E}[\sum_{i,j}ϵ_iA'_{ij}w_j] = \sum_{i,j}A'_{ij} \mathbb{E}[ϵ_iw_j] \underset{\epsilon, w \text{ are independent}}{=} \sum_{i,j}A'_{ij} \mathbb{E}[w_j] \mathbb{E}[ϵ_i]= 0
$$
Similarly, the third term is also zero. We'll analyze the first term using a similair method:
$$
\mathbb{E}[\epsilon^TXH_{\mu}^{-1}H_{\mu}^{-1}X^T\epsilon] \underset{A = XH_{\mu}^{-1}H_{\mu}^{-1}X^T}{=} \mathbb{E}[\epsilon^T A \epsilon] = \sum_{i,j}A_{ij} \mathbb{E}[ϵ_iϵ_j]
\\ \underset{i\ne j \text{ have covariance zero}}{=} \sum_{i\ne j}A_{ij} \mathbb{E}[ϵ_j] \mathbb{E}[ϵ_i] + \sum_{i=j}A_{ij} \mathbb{E}[ϵ_i^2] = \sigma_\epsilon^2 Tr(A)
$$
From here it's obvious that the fourth term will be $\mathbb{E}[w_{true}^T B w_{true}] = \frac{\sigma_w^2}{d} Tr(B)$ where $B = (H_{\mu}^{-1}H-I)^T (H_{\mu}^{-1}H-I)$. Using the properties of the trace of a product we get that $Tr(A) = Tr\left[XH_{\mu}^{-2}X^T \right] = Tr\left[H_{\mu}^{-2}X^TX \right] = Tr\left[XH_{\mu}^{-2}X^T \right] = Tr\left[H_{\mu}^{-2}H \right]$. Since $H$ and $H_{\mu}$ have the same eigenvectors, we use the fact the trace of a matrix is the sum of it's eigenvalues and get $Tr(A) = \sum_i \frac{\lambda_i}{(\lambda_i + \mu)^2}$. Furthermore, we can write $B=(HH_{\mu}^{-1}-I) (H_{\mu}^{-1}H-I)$ and we can see that $B$ has the same eigenvectors as matrix $H$, thus $B$'s eigenvalues are $\left( \frac{\lambda_i}{\mu + \lambda_i} - 1\right)^2 = \frac{\mu^2}{(\mu + \lambda_i)^2}$ and $Tr(B) = \sum_i \frac{\mu^2}{(\mu + \lambda_i)^2}$. Finally we have the required:
$$
\mathcal{\overline{R}}(w) = \sigma_\epsilon^2 Tr(A) + \frac{\sigma_w^2}{d} Tr(B) = \sum_{i=1}^d \frac{(\sigma_w^2/d) \mu^2 + \sigma_{\epsilon}^2 \lambda_i}{(\lambda_i + \mu)^2}
$$

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 2 - Backpropagation By Hand
---
Consider the following network:
<img src="https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png" style="height:300px">

We will work with one sample for this example, but it can be extended to mini-batches.

* Input: $x = \begin{bmatrix} 1 \\ 4 \\ 5 \end{bmatrix} \in \mathbb{R}^3$
* Output (target): $ t = \begin{bmatrix} 0.1 \\ 0.05 \end{bmatrix} \in \mathbb{R}^2 $
* Number of Hidden Layers: 1
* Activation: Sigmoid for both hidden and output layers
* Loss Functions: MSE

We initialize the weights and biases to random values as follows:
<img src="https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png" style="height:300px">

1. Perform one forward pass and calculate the MSE.
2. Perform backpropagation (one backward pass, i.e., calculate the gradients).
3. With a learning rate of $\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?

**Answer:**

1. We'll explictly calculate the required:
$$
h = \sigma(W_1x+b_1) = \sigma\left(\begin{bmatrix} 0.1 & 0.3 & 0.5 \\ 0.2 & 0.4 & 0.6 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \\ 5 \end{bmatrix} + \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} \right) = \sigma\left( \begin{bmatrix} 3.8 \\ 4.8 \end{bmatrix} \right) = \begin{bmatrix} 0.978 \\ 0.992 \end{bmatrix}
\\
o = \sigma(W_2h+b_2) = \sigma\left(\begin{bmatrix} 0.7 & 0.9 \\ 0.8 & 0.1 \end{bmatrix} \begin{bmatrix} 0.978 \\ 0.992 \end{bmatrix} + \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} \right) = \sigma\left(\begin{bmatrix} 2.0774 \\  2.3744 \end{bmatrix} \right) = \begin{bmatrix} 0.888 \\  0.915 \end{bmatrix}
\\
MSE = \left\|  \begin{bmatrix} 0.1 \\  0.05 \end{bmatrix} - \begin{bmatrix} 0.888 \\  0.915 \end{bmatrix} \right\| ^2 = 1.449
$$
2. We'll calculate the derivatives explicitly:
$$
\frac{d(MSE)}{do} = 2(o-t) = \begin{bmatrix} 0.788 \\  0.91 \end{bmatrix}
\\
\frac{d(MSE)}{d(W_2h+b_2)_i} = \frac{d(MSE)}{do}\frac{do}{d(W_2h+b_2)_i} = \begin{bmatrix} 0.788 \\  0.91 \end{bmatrix} \sigma(W_2h+b_2)(1-\sigma(W_2h+b_2)) =
$$

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 3 - Deep Double Descent
---

For the following plots:
1. Where is the critical point (the point of transition between the "Classical Regime" and "Modern Regime") of the deep double descent?
2. What type of double descent is shown (**look closely at the graph**)? Explain. There can be more than one correct answer.
    

a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style="height:300px">

b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style="height:400px">

c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style="height:300px">

**Answer:**

plot a:  
1. For the blue line the critical point is at about 200, and for the green line the critical point is at about 300.
2. The type of double descent that is shown is Model-wise. The Model-wise double descent occurs when after the critical regime, increasing the expressiveness of a model (in this case embedding dimension) results in better performance on the test set, and that is exactly the phenomenon we can see here.

plot b:
1. Thr critical regime for all plots is at about 10 to 15.
2. Again, the type of double descent that is shown is Model-wise. After the critical regime, increasing the expressiveness of a model (in this case the width parameter) results in better performance on the test set, which implies Model-wise double descent.

plot c:
 1. Notice that the x axis denotes the number of epochs. It seems that for the small model and the large model we did not reach a critical regime and for the large model it is just less than 100 epochs.
 2. The type of double descent that is shown is Epoch-wise. Epoch-wise double descent occurs when after the critical regime, increasing the training time leads results in better performance on the test set, and that is exactly the phenomenon we can see here.

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 4 - Initialization
---
Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \sigma_l =\frac{1}{\sqrt{\sum_j \mathbb{E} \left[\varphi^2(u_{l-1}[j])\right]}} $$
Show that for ReLU activation ($\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \sigma_l = \sqrt{\frac{2}{d_{l-1}}}$$

1. Under the assumption that the distribution of $W$ is symmetric ($\to$ the distribution of $u$ is symmetric).
2. Using the central limit theorem for large width.

Answer each section **separately** and assume the sections are independent.

All the notations are the same as in the lecture slides.

**Answer:**

1. As mentioned in the hint, Assuming $W$ is symmetric implies that $u$'s distribution is symmetric. Therefore we get:
$$
\mathbb{E} \left[\varphi^2(u_{l-1}[j])\right] = \mathbb{E} \left[\boldsymbol{1}_{u_{l-1}[j] \ge 0}\cdot u_{l-1}^2[j]\right]  \underset{\text{$u$'s distribution is symmetric}}{=} \frac{1}{2}Var(u_{l-1}[j]) = \frac{1}{2}
\\
\Rightarrow \sigma_l =\frac{1}{\sqrt{\sum_j \mathbb{E} \left[\varphi^2(u_{l-1}[j])\right]}} = \frac{1}{\sqrt{\sum_j \frac{1}{2}}} = \sqrt{\frac{2}{d_{l-1}}}
$$
Which is exactly the required.
2. As we showed in class, using CLT we get $\sigma_l =\frac{1}{\sqrt{d_{l -1}\mathbb{E}_{z \sim \mathcal{N}(0,1)} \left[\varphi^2(z)\right]}}$. And again, since $z \sim \mathcal{N}(0,1)$ has symmetric distribution we have:
$$
\mathbb{E}_{z \sim \mathcal{N}(0,1) } [\varphi^2(z)] = \mathbb{E}_{z \sim \mathcal{N}(0,1) } [\boldsymbol{1}_{z \ge 0}\cdot z^2] = \frac{1}{2}Var(z) = \frac{1}{2}
$$
and plugging this result back in the original term we get the required:
$$
\sigma_l = \frac{1}{\sqrt{\frac{1}{2}d_{l -1}}} = \sqrt{\frac{2}{d_{l-1}}}
$$

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 5 - Equivariance
---

Recall from lecture 6:
A function $f: \mathcal{R}^d \to \mathcal{R}$ is equivariant if $f(\tau \cdot x) = \tau \cdot f(x)$ for all $\tau$.

Let $f_w(x) = \phi (Wx)$ where $\phi$ is a component-wise non-linearity and $W \in \mathcal{R}^{d\times d}$. Prove that $f_w:\mathcal{R}^d \to \mathcal{R}^d$ is equivariant to transformation family $H$ **if and only if**: $$ \forall \tau \in H, W[i, j] = W[\tau(i), \tau(j)] $$

* Assume one-by-one activations (<a href="https://en.wikipedia.org/wiki/Injective_function">Injective functions/one-by-one</a>)

**Answer:**

As discussed in class, we assume that $\tau$ is a bijective mapping of the indexes. As implied in the question, for $i\in \mathbb{N}$ we'll denote $\tau(i)$ as the index for which holds $\tau(e_i) = e_{\tau(i)}$ (where $e_i$ is a standard basis vector).

Assume $f_w:\mathcal{R}^d \to \mathcal{R}^d$ is equivariant to transformation family $H$. Let $\tau \in H$ and let $e_i \in \mathcal{R}^d$ a standard basis vector. Since $\sigma$ is component wise and surjective, we can observe:
$$
[\sigma(W\tau e_j)]_{\tau(i)} = [\tau\sigma(W e_j)]_{\tau(i)} \Leftrightarrow \sigma(W[\tau(i), \tau(j)]) = \sigma(W[i,j]) \Leftrightarrow W[\tau(i), \tau(j)] = W[i,j]
$$
Now assume $\forall \tau \in H: W[i, j] = W[\tau(i), \tau(j)]$. Again, using the fact that the activation function is piecewise we get the required:
$$
[\tau\sigma(W x)]_{\tau(i)} = \sigma\left(\sum_{k=1}^d W[i,k] x[k]\right) = \sigma\left(\sum_{k=1}^d W[\tau(i),\tau(k)] x[k]\right) =  [\sigma(W\tau x)]_{\tau(i)}
$$

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 6 -VGG Architecture
---

1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.


* CONV$M$-$N$: a convolutional layer of size $M \times M \times N$, where $M$ is the kernel size and $N$ is the number of filters. $stride=1, padding=1$.
* POOL2: $2 \times 2$ Max Pooling with $stride=2$
    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \times 3.5 \times 3$, you should round to $3 \times 3 \times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).
* FC-N: a fully connected layer with $N$ neurons.


| Layer  | Output Dimension  | Number of Parameters (Weights) |
|---|---|---|
| INPUT  |  224x224x3 | 0  |
|  CONV3-64 | 224x224x64  | 1792 |
| ReLU | 224x224x64 | 0 |
| POOL2|  112x112x64 | 0  |
|CONV3-128 | 112x112x128 | 73,856 |
|ReLU | 112x112x128 | 0 |
| POOL2|  56x56x128 | 0 |
|CONV3-256 | 56x56x256 | 295,168 |
|ReLU | 56x56x256 | 0 |
|CONV3-256 | 56x56x256 | 590,080|
|ReLU | 56x56x256 | 0 |
| POOL2| 23x23x256 | 0 |
|CONV3-512 | 23x23x512 | 1,180,160 |
|ReLU | 23x23x512 | 0 |
|CONV3-512 | 23x23x512 | 2,359,808 |
|ReLU | 23x23x512 | 0 |
| POOL2|  11x11x512 | 0 |
|CONV3-512 | 11x11x512 |  2,359,808 |
|ReLU | 11x11x512 | 0|
|CONV3-512 | 11x11x512 | 2,359,808 |
|ReLU | 11x11x512 | 0 |
| POOL2|  5x5x512 | 0 |
| FC-4096|  4096 | 52,432,896 |
| FC-4096|  4096 | 16,781,312 |
| FC-1000|  1000 | 4,097,000 |
| SOFTMAX|  1000 | 0 |

2. What is the total number of parameters? (use a calculator for this one)
3. What percentage of the weights are found in the fully-connected layers?

**Answer:**
1. The table in the bidy of the question is filled in.
2. The total number of parameters is 82,531,688.
3. The number of weights in the fully connected layers is 73,311,208. Thus the percentage is 88.8%.

### <img src="https://img.icons8.com/officel/80/000000/code.png" style="height:50px;display:inline"> Part 2 - Code Assignments
---
* You must write your code in this notebook and save it with the output of all of the code cells.
* Additional text can be added in Markdown cells.
* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output.

#### Tips
---
1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`
2. Separation to **validation set** in PyTorch - <a href="https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123">See example here</a>.
"""

# !pip install kornia

# Commented out IPython magic to ensure Python compatibility.
# imports for the practice (you can add more if you need)
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, random_split, ConcatDataset, Subset
import torchvision
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import torch.nn.init as init
import time
from torchvision.transforms import ToTensor
# %matplotlib notebook
# %matplotlib inline

import kornia
import kornia.feature as KF
from kornia import augmentation as A
import random
import torchvision.transforms as transforms
seed = 211
np.random.seed(seed)
torch.manual_seed(seed)

"""### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 1 - The Importance of Activation and Initialization
---
In this task, we are going to use $x \in \mathcal{R}^{512}$ and simple neural network that outputs $f(x) \in \mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.

1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.
2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \hat{std} = \sqrt{\frac{\sum_{i=1}^{10000}\frac{1}{N}\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.
3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.
4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.
5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\pm \sqrt{\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.
6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\sqrt{\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?
"""

input_size = 512
x = torch.randn(input_size)
for i in range(100):
    a = torch.randn(input_size, input_size)
    x = a @ x
    if torch.any(torch.isnan(x)):
      print("layer number is : " + str(i))
      break
print(x.mean(), x.std())

"""Your answers here

A1.

Since A is initial with rand values, we get to a point( layer 28) in which the values are too high, leading to overflow and cause the values to be "nan"
####  
---
"""

# part 2
sum = 0
all_std = []
for i in range (10000):
  x = torch.randn(input_size)
  a = torch.randn(input_size, input_size)
  y = a @ x
  current_sum = torch.sum(y**2)/input_size
  sum += (current_sum)
  all_std.append(current_sum.item())


std = np.sqrt(sum/10000)
mean = torch.tensor(all_std).mean().item()
square_root_input = torch.sqrt(torch.tensor(input_size))


print("Mean: ", mean)
print("Empirical standard deviations: ", std)
print("Square root of the number of input connections:", square_root_input)

"""####  
---
"""

# part 3
x = torch.randn(input_size)
for i in range(100):
    a = torch.randn(input_size, input_size)
    a /= torch.sqrt(torch.tensor(input_size))
    x = a @ x
    if torch.any(torch.isnan(x)):
      print("layer number is : " + str(i))
      break
print(x.mean(), x.std())

"""A3.

We see that now with normalization the output is not nan, meaning we havn't got to overflow.

####  
---
"""

# part 4
x = torch.randn(input_size)
for i in range(100):
    a = torch.randn(input_size, input_size)
    x = a @ x
    x= torch.tanh(x)
print(x.mean(), x.std())

"""A4.

We see that the added layers of tanh() replace the normalization, and keeps the output of each layer in the range of [-1,1] , meaning we will never get overflow.
####  
---
"""

# part 5
def xavier_init(fan_in, fan_out) :
  bound = np.sqrt(6/(fan_in + fan_out))
  ret = torch.rand(fan_in, fan_in)
  ret = ret*2*bound - bound
  return ret


x = torch.randn(input_size)
for i in range(100):
    a = xavier_init(input_size, input_size)
    x = a @ x
    x= torch.tanh(x)
print(x.mean(), x.std())

"""####  
---
"""

# part 6

def kaiming_init(fan_in, fan_out) :
  stddev = np.sqrt(2/fan_in)
  ret = torch.normal(0, stddev, size=(fan_in, fan_in))
  return ret


x = torch.randn(input_size)
for i in range(100):
    a = kaiming_init(input_size, input_size)
    x = a @ x
    x= torch.relu(x)
print(x.mean(), x.std())

"""A6.
Using Relu without kaiming_init nor xavier_initwe get overflow.
When using it with xavier, X gets to 0..

"""

# xavier + relu
x = torch.randn(input_size)
for i in range(100):
    a = xavier_init(input_size, input_size)
    x = a @ x
    x= torch.relu(x)
print(x.mean(), x.std())

"""####  
---

### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 2 - MLP-based Deep Classifer
---
In this task you are going to design and train your first neural network for classification.

For this task, we will use the "<a href="https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope">MAGIC Gamma Telescope Data Set"</a>. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera.

Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (**signal**) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (**background**).

Our data has 10 features and 2 classes (signal and background).

1. Load the MAGIC dataset sored in `magic04.data` and display the first 5 features (just run the cell).
2. Separate the data to train, validation and test, reserve 10% of the data for validation and 20% for test.
3. Perform pre-processing steps of your choice and convert the class label from `str` to `int` (for example, `y_train = np.array([0 if y_train[i] == 'g' else 1 for i in range(len(y_train))]).astype(np.int)`).
4. Train a Logistic Regression model from `sklearn` as a baseline for our neural network (only for this section use both the train and validation sets for training the classifier). **Print the test accuracy**.
5. Convert the `numpy` arrays to `torch` tensors with `TensorDataset` as done in the tutorial.
6. Design a **MLP** to classify the data. Optimize the hyper-parameters of your model using the accuracy on the validation set, and when you are satisfied with the model train it on both the train and validation sets and evaluate it on the test set. **You need to reach at least 85% accuracy on the test set, and 87% for a full grade**.
    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.
    * The loss criterion is binary cross entropy: `nn.BCEWithLogitsLoss()` (performs `sigmoid` for you) or `nn.BCELoss` (you need to apply `sigmoid` on the network output yourself).
    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.
        * Make sure to describe any design choice that you used to improve the performance (e.g. if you used a certain regularization or layer, mention it and describe why you think it helped).
    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations. **Print** the final performance.
    * **Print** the test accuracy.
7. Pick **2** initializations of your choosing and change the initialization of the linear layers and re-train the model (with the same optimal hyper-parameters you found). You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy for both changes (you should end up with 3 results - original, `init 1` and `init 2`).
"""

# loading the data
col_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
feature_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist']
data = pd.read_csv("./magic04.data", names=col_names)
X = data[feature_names]
Y = data['class']
data.head()

# separate to train, test
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed, stratify=Y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1/0.8, random_state=seed, stratify=y_train)

# pre-processing and converting labels to integers
X_scaler = StandardScaler()
X_scaler.fit(X_train)
X_train = X_scaler.transform(X_train)
X_val = X_scaler.transform(X_val)
X_test = X_scaler.transform(X_test)

y_train = np.where(y_train == "g", 0, 1)
y_val = np.where(y_val == "g", 0, 1)
y_test = np.where(y_test == "g", 0, 1)

# training a Logistic Regression baseline - complete the code with your variables
X_train_prep = np.concatenate((X_train , X_val))
X_test_prep = X_test
y_train_np = np.concatenate((y_train , y_val))
y_test_np = y_test
logstic_model = LogisticRegression(solver='lbfgs')
y_pred = logstic_model.fit(X_train_prep, y_train_np).predict(X_train_prep)
print("Number of mislabeled points %d out of %d total points."% ((y_train_np != y_pred).sum(), X_train.shape[0]))
print("Logistic Regression Model accuracy =" , logstic_model.score(X_test_prep, y_test_np))

# create TensorDataset from numpy arrays
tensor_train = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))

# model, hyoer-paramerters and training

class MyModel(nn.Module):
    def __init__(self, input_dim, output_dim, dropout_prob, hidden_dimA=128, hidden_dimB = 64,hidden_dimC = 32, negative_slope = 0.1 ):
        super(MyModel, self).__init__()
        self.hidden = nn.Sequential(nn.Dropout(p=dropout_prob),
                                    nn.Linear(input_dim, hidden_dimA),
                                    nn.LeakyReLU(negative_slope=negative_slope),
                                    nn.Linear(hidden_dimA, hidden_dimB),
                                    nn.LeakyReLU(negative_slope=negative_slope),
                                    nn.Dropout(p=dropout_prob),
                                    nn.Linear(hidden_dimB, hidden_dimC),
                                    nn.LeakyReLU(negative_slope=negative_slope),
                                    nn.Dropout(p=dropout_prob),
                                    )
        self.output_layer = nn.Linear(hidden_dimC, output_dim)

        # for layer in [self.hidden[0], self.hidden[2], self.hidden[4],self.hidden[7],self.hidden[9] , self.hidden[12], self.hidden[14], self.output_layer]:
        #     if isinstance(layer, nn.Linear):
        #         init.xavier_uniform_(layer.weight)

    def forward(self, x):
        # here we define what happens to the input x in the forward pass
        # that is, the order in which x goes through the building blocks
        return self.output_layer(self.hidden(x))


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        torch.nn.init.xavier_normal_(m.weight, gain=1.0)



num_features = 10
output_dim = 1

# step_size=350
# gamma=0.95
step_size=1600
gamma=0.9506711626187144
batch_size = 64
num_epochs = 1600

learning_rate = 0.002549511409850021
dropout_prob=0.3123430608173919
weight_decay = 0.0012877538221889087
hidden_dimA = 256
hidden_dimB = 128
hidden_dimC = 64
negative_slope = 0.05853227689790635

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

criterion = nn.BCEWithLogitsLoss()

model = MyModel(num_features, output_dim, dropout_prob, hidden_dimA,hidden_dimB, hidden_dimC ).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma = gamma)

tensor_train_dataloader = DataLoader(tensor_train, batch_size=batch_size, shuffle=True)

model.apply(weights_init)


for epoch in range(num_epochs):
    model.train()  # change the mode to training, activating layers like DropOut and BatchNorm, if there are any
    epoch_losses = []
    for features, targets in tensor_train_dataloader:
        # send data to device
        features = features.to(device)
        targets = targets.to(device)
        # forward pass
        output = model(features)  # calls model.forward(features)
        # loss
        loss = criterion(output.view(-1), targets)
        # backward pass
        optimizer.zero_grad()  # clean the gradients from previous iteration, clears the `tensor.grad` field (tensor.grad=0)
        loss.backward()  # autograd backward to calculate gradients, assigns the `tensor.grad` field (e.g., tensor.grad=0.27)
        optimizer.step()  # apply update to the weights, applies the gradient update rule of the optimizer (param=param - lr * grad)
        scheduler.step()
        epoch_losses.append(loss.item())
    if epoch % 50 == 0:
        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')
        print(f'Epoch [{epoch}/{num_epochs}], Learning Rate: {optimizer.param_groups[0]["lr"]:.7f}')
        model.eval()
        with torch.no_grad():
          val_outputs = model(torch.tensor(X_val, dtype=torch.float, device=device))
          val_error = criterion(val_outputs.view(-1), torch.tensor(y_val, dtype=torch.float, device=device))
        print(f'val binary cross entropy error: {val_error.item()}')


# Print or record current hyperparameter combination and corresponding metrics
print(f"LR: {learning_rate}, Dropout: {dropout_prob}, Hidden Dim: {hidden_dim}, WD: {weight_decay}, Epoch: {epoch}, Loss: {np.mean(epoch_losses)}")

# Define hyperparameter ranges
learning_rates = [0.001, 0.01, 0.05]
dropout_probs = [0.2, 0.3, 0.4]
hidden_dims = [64, 128, 256]
weight_decays = [0.0001, 0.001, 0.01]
num_epochs = 800
step_size=800
batch_sizes = [32,64, 128, 256,512]

# Loop through hyperparameter combinations
for _ in range (20):

  lr = np.random.uniform(0.001, 0.01)
  dropout_prob =np.random.uniform(0.3,0.5)
  weight_decay = np.random.uniform(0.0005, 0.002)
  gamma = np.random.uniform(0.94,0.96)
  # batch_size = np.random.choice(hidden_dims)
  batch_size = 128
  negative_slope = np.random.uniform(0.05,0.2)
  hidden_dimA = np.random.choice(hidden_dims)
  hidden_dimB = int(hidden_dimA/2)
  hidden_dimC = int(hidden_dimB/2)
  batch_size = int(np.random.choice(batch_sizes))


  model = MyModel(num_features, output_dim, dropout_prob, hidden_dimA=hidden_dimA,hidden_dimB=hidden_dimB ).to(device)
  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
  criterion = nn.BCEWithLogitsLoss()
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma = gamma)
  tensor_train_dataloader = DataLoader(tensor_train, batch_size=batch_size, shuffle=True)
  model.apply(weights_init)

  for epoch in range(num_epochs+1):
    model.train()  # change the mode to training, activating layers like DropOut and BatchNorm, if there are any
    epoch_losses = []
    for features, targets in tensor_train_dataloader:
        # send data to device
        features = features.to(device)
        targets = targets.to(device)
        # forward pass
        output = model(features)  # calls model.forward(features)
        # loss
        loss = criterion(output.view(-1), targets)
        # backward pass
        optimizer.zero_grad()  # clean the gradients from previous iteration, clears the `tensor.grad` field (tensor.grad=0)
        loss.backward()  # autograd backward to calculate gradients, assigns the `tensor.grad` field (e.g., tensor.grad=0.27)
        optimizer.step()  # apply update to the weights, applies the gradient update rule of the optimizer (param=param - lr * grad)
        scheduler.step()
        epoch_losses.append(loss.item())
    if epoch % 50 == 0:
        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')
        print(f'Epoch [{epoch}/{num_epochs}], Learning Rate: {optimizer.param_groups[0]["lr"]:.7f}')

    if epoch % 100 == 0:
       model.eval()
       with torch.no_grad():
         val_outputs = model(torch.tensor(X_val, dtype=torch.float, device=device))
         val_error = criterion(val_outputs.view(-1), torch.tensor(y_val, dtype=torch.float, device=device))
       print(f'val binary cross entropy error: {val_error.item()}')


  # Print or record current hyperparameter combination and corresponding metrics
  print(f"LR: {lr}, Dropout: {dropout_prob}, Hidden DimA: {hidden_dimA}, Hidden DimB: {hidden_dimB} , WD: {weight_decay}, gamma: {gamma},slope: {negative_slope}, Loss: {np.mean(epoch_losses)}")

# val error
model.eval()
with torch.no_grad():
    val_outputs = model(torch.tensor(X_val, dtype=torch.float, device=device))
    val_error = criterion(val_outputs.view(-1), torch.tensor(y_val, dtype=torch.float, device=device))
print(f'val binary cross entropy error: {val_error.item()}')

# # example of weight initialization
# import torch.nn as nn
# class MyModel(nn.Module):
#     def __init__(self, parmaeters):
#         super(MyModel, self).__init__()
#         # model definitions/blocks
#         # ...
#         # custom initialization
#         self.init_weights()

#     def init_weights(self):
#         for m in self.modules():
#             if isinstance(m, nn.Linear):
#                 # pick initialzation: https://pytorch.org/docs/stable/nn.init.html
#                 # examples
#                 # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
#                 # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))
#                 # nn.init.normal_(m.weight, 0, 0.005)
#                 # don't forget the bias term (m.bias)

#     def forward(self, x):
#         # ops on x
#         # ...
#         # output = f(x)
#         return output

"""### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 3 - Design a CNN
---
In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset.

SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.

* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...
* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.

<img src="http://ufldl.stanford.edu/housenumbers/32x32eg.png" style="height:250px">

1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/vision/stable/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN. Display 5 images from the train set.
2. Design a Convolutional Neural Network (CNN) to classify digits from the images.
    * You are **not allowed** to use `BatchNorm` in your architecture, but can use any other normalization (`GroupNorm`, `LayerNorm`, and etc..).
    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?
    * What is the input dimension? What is the output dimension?
    * Calculate the number of parameters (weights) in the network. What is the model size in MegaBytes (MB)? (see the convolution tutorial). **Print** these numbers.
4. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).
    * **DO NOT USE ANY IMAGE AUGMENTATIONS IN THIS PART** (You can still use `Normalize` if you wish, but no cropping, flipping and etc...).
    * Describe the the hyper-parameters of the model (batch size, epochs, optimizer, learning rate, scheduler....). How did you tune your model? Did you use a validation set to tune the model?
    * What is the final accuracy on the test set? **Print** it.
        * You need to reach at least 86% accuracy in this section, and 89% for maximum points in section 4.
    * **Plot** the loss curves (and any other statistic you calculate) as a function of epochs/iterations.
6. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \text{image} + a \times \mathcal{N}(0, 1).$$ **Print** the result for each value of $a$.
7. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.
    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.
    * You are welcome to use <a href="https://kornia.github.io/">`kornia`</a> for the augmentations (**2 points bonus**, maximal grade is still 100).
    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.
"""

trainsetraw = torchvision.datasets.SVHN(root='./data', split='train', transform=None, target_transform=None, download=True)
testsetraw = torchvision.datasets.SVHN(root='./data', split='test', transform=None, target_transform=None, download=True)

for i in range(5):
    image, label = trainsetraw[i]
    plt.subplot(1, 5, i + 1)
    plt.imshow(image)
    plt.axis('off')

plt.show()

class CNN(nn.Module):

    def __init__(self,kernel_size=3 ,Dropout = 0.1):
        """CNN Builder."""
        super(CNN, self).__init__()

        self.conv_layer = nn.Sequential(

            # Conv Layer block 1
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=kernel_size, padding='same'),
            # `paading=1` is the same as `padding='same'` for 3x3 kernels size
            nn.GroupNorm(num_groups=1, num_channels=32),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size, padding='same'),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 2 = 32 / 2 = 16

            # Conv Layer block 2
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel_size, padding='same'),
            nn.GroupNorm(num_groups=1, num_channels=128),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=kernel_size, padding='same'),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 4 = 32 / 4 = 8
            nn.Dropout2d(p=0.05),

            # Conv Layer block 3
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=kernel_size, padding='same'),
            nn.GroupNorm(num_groups=1, num_channels=256),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=kernel_size, padding='same'),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 8 = 32 / 8 = 4
            # the output dimensions: [batch_size, 256, h=input_resolution / 8, w=input_resolution / 8]
        )


        self.fc_layer = nn.Sequential(
            nn.Dropout(p=Dropout),
            nn.Linear(4096, 1024),  # 256 * 4 * 4 = 4096
            nn.ReLU(inplace=True),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=Dropout),
            nn.Linear(512, 10)
        )


    def forward(self, x):
        """Perform forward."""

        # conv layers
        x = self.conv_layer(x)  # [batch_size, channels=256, h_f=4, w_f=4]

        # flatten - can also use nn.Flatten() in __init__() instead
        x = x.view(x.size(0), -1)  # [batch_size, channels * h_f * w_f=4096]

        # fc layer
        x = self.fc_layer(x)  # [batch_size, n_classes=10]

        return x

# how can we calcualte the output of the convolution automatically?
dummy_input = torch.zeros([1, 3, 32, 32])
dummy_model = CNN()
dummy_output = dummy_model.conv_layer(dummy_input)
print(dummy_output.shape)
dummy_output = dummy_output.view(dummy_output.size(0), -1)
print(dummy_output.shape)
# how many weights (trainable parameters) we have in our model?
num_trainable_params = sum([p.numel() for p in dummy_model.parameters() if p.requires_grad])
print("num trainable weights: ", num_trainable_params)

# calculate the model size on disk
num_trainable_params = sum([p.numel() for p in dummy_model.parameters() if p.requires_grad])
param_size = 0
for param in dummy_model.parameters():
    param_size += param.nelement() * param.element_size()
buffer_size = 0
for buffer in dummy_model.buffers():
    buffer_size += buffer.nelement() * buffer.element_size()
size_all_mb = (param_size + buffer_size) / 1024 ** 2
print(f"model size: {size_all_mb:.2f} MB")

to_tensor = ToTensor()

train_size = int(0.85 * len(trainsetraw))  # 85% of the dataset for training
valid_size = len(trainsetraw) - train_size  # Remaining 15% for validation

# Split the dataset into training and validation sets
trainset, validset = random_split(trainsetraw, [train_size, valid_size])
trainProcessed = [(to_tensor(image), label) for image, label in trainset]
valProcessed = [(to_tensor(image), label) for image, label in validset]
testProcessed = [(to_tensor(image), label) for image, label in testsetraw]

# hyper-parameters
batch_size = 128
learning_rate = 1e-4
epochs = 21
Dropout = 0.1


# dataloaders - creating batches and shuffling the data
trainloader = torch.utils.data.DataLoader(
   trainProcessed, batch_size=batch_size, shuffle=True, num_workers=2)
valloader = torch.utils.data.DataLoader(
    valProcessed, batch_size=batch_size, shuffle=False, num_workers=2)
testloader = torch.utils.data.DataLoader(
    testProcessed, batch_size=batch_size, shuffle=False, num_workers=2)
# device - cpu or gpu?
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# loss criterion
criterion = nn.CrossEntropyLoss()  # accepts 'logits' - unnormalized scores (no need to apply `softmax` manually)

# build our model and send it to the device
model = CNN().to(device)

# optimizer - SGD, Adam, RMSProp...
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

def calculate_accuracy(model, dataloader, device):
    model.eval() # put in evaluation mode,  turn off Dropout, BatchNorm uses learned statistics
    total_correct = 0
    total_images = 0
    confusion_matrix = np.zeros([10, 10], int)
    with torch.no_grad():
        for data in dataloader:
            images, labels = data
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total_images += labels.size(0)
            total_correct += (predicted == labels).sum().item()
            for i, l in enumerate(labels):
                confusion_matrix[l.item(), predicted[i].item()] += 1

    model_accuracy = total_correct / total_images * 100
    return model_accuracy, confusion_matrix

train_accuracys=[]
val_accuracys = []
losses=[]
for epoch in range(1, epochs + 1):
    model.train()  # put in training mode, turn on Dropout, BatchNorm uses batch's statistics
    running_loss = 0.0
    epoch_time = time.time()
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data
        # send them to device
        inputs = inputs.to(device)
        labels = labels.to(device)
        # augmentation with `kornia` happens here inputs = aug_list(inputs)

        # forward + backward + optimize
        outputs = model(inputs)  # forward pass
        loss = criterion(outputs, labels)  # calculate the loss
        # always the same 3 steps
        optimizer.zero_grad()  # zero the parameter gradients
        loss.backward()  # backpropagation
        optimizer.step()  # update parameters

        # print statistics
        running_loss += loss.data.item()

    # Normalizing the loss by the total number of train batches
    running_loss /= len(trainloader)

    # Calculate training/validation set accuracy of the existing model
    train_accuracy, _ = calculate_accuracy(model, trainloader, device)
    val_accuracy, _ = calculate_accuracy(model, valloader, device)

    train_accuracys.append(train_accuracy)
    val_accuracys.append(val_accuracy)
    losses.append(running_loss)

    log = "Epoch: {} | Loss: {:.4f} | Training accuracy: {:.3f}% | validation accuracy: {:.3f}% | ".format(epoch, running_loss, train_accuracy, val_accuracy)
    # with f-strings
    # log = f"Epoch: {epoch} | Loss: {running_loss:.4f} | Training accuracy: {train_accuracy:.3f}% | validation accuracy: {val_accuracy:.3f}% |"
    epoch_time = time.time() - epoch_time
    log += "Epoch Time: {:.2f} secs".format(epoch_time)
    # with f-strings
    # log += f"Epoch Time: {epoch_time:.2f} secs"
    print(log)

    # save model
    if epoch % 10 == 0:
        print('==> Saving model ...')
        state = {
            'net': model.state_dict(),
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoints'):
            os.mkdir('checkpoints')
        torch.save(state, './checkpoints/cnn_ckpt.pth')

print('==> Finished Training ...')

test_accuracy, _ = calculate_accuracy(model, testloader, device)
log = " test accuracy: {:.3f}% | ".format( test_accuracy)
print(log)

plt.figure(figsize=(10, 7))
plt.plot(train_accuracys, label='train accuracy')
plt.plot(val_accuracys, label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Epoch')
plt.legend()
plt.show()

plt.figure(figsize=(10, 7))
plt.plot(losses, label='loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')
plt.legend()
plt.show()

# load model, calculate accuracy and confusion matrix
model = CNN().to(device)
state = torch.load('./checkpoints/cnn_ckpt.pth', map_location=device)
model.load_state_dict(state['net'])
# note: `map_location` is necessary if you trained on the GPU and want to run inference on the CPU

test_accuracy, confusion_matrix = calculate_accuracy(model, testloader, device)
print("test accuracy: {:.3f}%".format(test_accuracy))

# plot confusion matrix
fig, ax = plt.subplots(1,1,figsize=(8,6))
ax.matshow(confusion_matrix, aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))
plt.ylabel('Actual Category')
plt.yticks(range(10))
plt.xlabel('Predicted Category')
plt.xticks(range(10))
plt.show()

a = [0.05, 0.01, 0.005]
to_tensor = ToTensor()
for noise in a:
  testProcessed_noise = [(np.array(image) + noise * 255*np.random.randn(*np.array(image).shape), label) for image, label in testset]
  testProcessed_noise_tensor = [(to_tensor(image).float(), label) for image, label in testProcessed_noise]
  testloader_noise = torch.utils.data.DataLoader(testProcessed_noise_tensor, batch_size=batch_size, shuffle=True, num_workers=2)
  test_accuracy_noise, _ = calculate_accuracy(model, testloader_noise, device)
  log = "noise added using a = {:.3f}% , test accuracy: {:.3f}% | ".format( noise, test_accuracy_noise)
  print(log)

image, label = trainset[0]
trans = transform(to_tensor(image))

plt.subplot(1, 2, 1)
plt.imshow(image)
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(kornia.tensor_to_image(trans))
plt.axis('off')

plt.show()

transform_kornia = A.AugmentationSequential(
    A.RandomRotation(degrees=(-10, 10)),
    A.ColorJitter(0.2, 0.2, 0.2, 0.2),
    A.RandomGrayscale(p=0.1),
    A.RandomAffine(degrees=0, shear=(-0.1, 0.1)),  # Minor change in argument name
    A.RandomPerspective(0.1),
)

transform_torchvision = ToTensor()

# Compose the transformations
transform = transforms.Compose([transform_torchvision, transform_kornia])
trainset = torchvision.datasets.SVHN(root='./data', split='train', transform=transform, download=True)
trainloader = torch.utils.data.DataLoader(
   trainProcessed, batch_size=batch_size, shuffle=True, num_workers=2)

train_accuracys=[]
val_accuracys = []
losses=[]
for epoch in range(1, epochs + 1):
    model.train()  # put in training mode, turn on Dropout, BatchNorm uses batch's statistics
    running_loss = 0.0
    epoch_time = time.time()
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data
        # send them to device
        inputs = inputs.to(device)
        labels = labels.to(device)
        # augmentation with `kornia` happens here inputs = aug_list(inputs)

        # forward + backward + optimize
        outputs = model(inputs)  # forward pass
        loss = criterion(outputs, labels)  # calculate the loss
        # always the same 3 steps
        optimizer.zero_grad()  # zero the parameter gradients
        loss.backward()  # backpropagation
        optimizer.step()  # update parameters

        # print statistics
        running_loss += loss.data.item()

    # Normalizing the loss by the total number of train batches
    running_loss /= len(trainloader)

    # Calculate training/validation set accuracy of the existing model
    train_accuracy, _ = calculate_accuracy(model, trainloader, device)
    val_accuracy, _ = calculate_accuracy(model, valloader, device)

    train_accuracys.append(train_accuracy)
    val_accuracys.append(val_accuracy)
    losses.append(running_loss)

    log = "Epoch: {} | Loss: {:.4f} | Training accuracy: {:.3f}% | validation accuracy: {:.3f}% | ".format(epoch, running_loss, train_accuracy, val_accuracy)
    # with f-strings
    # log = f"Epoch: {epoch} | Loss: {running_loss:.4f} | Training accuracy: {train_accuracy:.3f}% | validation accuracy: {val_accuracy:.3f}% |"
    epoch_time = time.time() - epoch_time
    log += "Epoch Time: {:.2f} secs".format(epoch_time)
    # with f-strings
    # log += f"Epoch Time: {epoch_time:.2f} secs"
    print(log)

    # save model
    if epoch % 10 == 0:
        print('==> Saving model ...')
        state = {
            'net': model.state_dict(),
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoints'):
            os.mkdir('checkpoints')
        torch.save(state, './checkpoints/cnn_ckpt.pth')

print('==> Finished Training ...')

"""## <img src="https://img.icons8.com/dusk/64/000000/prize.png" style="height:50px;display:inline"> Credits
---
* Icons made by <a href="https://www.flaticon.com/authors/becris" title="Becris">Becris</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a>
* Icons from <a href="https://icons8.com/">Icons8.com</a> - https://icons8.com
* Datasets from <a href="https://www.kaggle.com/">Kaggle</a> - https://www.kaggle.com/
"""