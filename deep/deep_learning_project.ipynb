{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dL40voW92Y4M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import  AdamW\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import math\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import BertTokenizer\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class PositionalEncoding(nn.Module):\n",
        "\n",
        "#     def __init__(self, d_model, dropout=0.1, max_len=24000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.pe[:x.size(0), :]\n",
        "#         return self.dropout(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=23187):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        pe = torch.zeros((max_len, d_model)) \n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "class TransformerTextClassifier(nn.Module):\n",
        "    def __init__(self,ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout=0.5, norm_first=True):\n",
        "        super(TransformerTextClassifier, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.fc = nn.Linear(ninp, num_classes-1)\n",
        "        self.activation =nn.GELU() \n",
        "        #self.activation =nn.LeakyReLU()\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = output.mean(dim=1)  # Pooling layer (e.g., mean pooling)\n",
        "        output = self.fc(output)\n",
        "        # output = torch.sigmoid(output)  \n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    # def init_weights(self):\n",
        "    #     initrange = 0.1\n",
        "    #     self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    #     self.fc.bias.data.zero_()\n",
        "\n",
        "\n",
        "\n",
        "class IDDataset(Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id = self.input_ids[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return {\n",
        "            'input_ids': id,\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, token_ids, labels):\n",
        "        self.token_ids = token_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_id = self.token_ids[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_id, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n2M31yb62Msu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(txt:str):\n",
        "\n",
        "    txt = re.sub('[^a-zA-Z]', ' ', txt)\n",
        "    txt = txt.lower()\n",
        "    txt = \" \".join(txt.split())\n",
        "\n",
        "    doc = nlp(txt)\n",
        "\n",
        "    tokens_filtered = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "\n",
        "        tokens_filtered.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(tokens_filtered)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def tokenize_and_convert(text):\n",
        "    # Tokenize the text using spaCy\n",
        "    spaCy_tokens = [token.text for token in nlp(text)]\n",
        "    \n",
        "    # Convert spaCy tokens to strings\n",
        "    token_strings = [str(token) for token in spaCy_tokens]\n",
        "    \n",
        "    # Map token strings to numerical IDs using the pre-trained tokenizer\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(token_strings)\n",
        "    \n",
        "    return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "678aczQf2PIB",
        "outputId": "9ed79bc5-8652-42ec-8789-6c9dd9bff2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text         label\n",
            "0  I recently went through a breakup and she said...    depression\n",
            "1  I do not know how to navigate these feelings, ...    depression\n",
            "2  So I have been with my bf for 5 months , and h...    depression\n",
            "3  I am so exhausted of this. Just when I think I...  SuicideWatch\n",
            "4  I have been severly bullied since i was 5 till...    depression\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       count    %\n",
              "text       0  0.0\n",
              "label      0  0.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the CSV file\n",
        "data = pd.read_csv('mental-health.csv')\n",
        "# Display the first few rows of the DataFrame\n",
        "print(data.head())\n",
        "data = data.drop_duplicates(ignore_index = True)\n",
        "df_null_values = data.isnull().sum().to_frame().rename(columns = {0:'count'})\n",
        "df_null_values['%'] = (df_null_values['count'] / len(data)) * 100\n",
        "df_null_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3QsbkKPE2hSx"
      },
      "outputs": [],
      "source": [
        "data['text_prep'] = data['text'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['token_id'] =  data['text_prep'].apply(tokenize_and_convert)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xwa4RZI52qw-"
      },
      "outputs": [],
      "source": [
        "LABELS = data['label'].unique()\n",
        "label2id = dict(zip(LABELS, np.arange(len(LABELS), dtype = np.float32)))\n",
        "data['label_prep'] = data['label'].map(label2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.dropna(subset=['text_prep'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.to_csv('preprocessed_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "##########\n",
        "## load data if the preprocessed instead of reprocessed\n",
        "data = pd.read_csv('preprocessed_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data[data['token_id'].isna()]['token_id'].head()\n",
        "# data['token_id'] = data['token_id'].fillna('[]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_string_to_array(s):\n",
        "    return np.array(ast.literal_eval(s))\n",
        "\n",
        "data['token_id']  = data['token_id'] .apply(convert_string_to_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_prep</th>\n",
              "      <th>token_id</th>\n",
              "      <th>label_prep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I recently went through a breakup and she said...</td>\n",
              "      <td>depression</td>\n",
              "      <td>recently go breakup say want friend say try ta...</td>\n",
              "      <td>[3728, 2175, 19010, 2360, 2215, 2767, 2360, 30...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I do not know how to navigate these feelings, ...</td>\n",
              "      <td>depression</td>\n",
              "      <td>know navigate feeling new feeling stretch unde...</td>\n",
              "      <td>[2113, 22149, 3110, 2047, 3110, 7683, 3305, 27...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I have been with my bf for 5 months , and h...</td>\n",
              "      <td>depression</td>\n",
              "      <td>bf month tell depressed week particular happen...</td>\n",
              "      <td>[28939, 3204, 2425, 14777, 2733, 3327, 4148, 2...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am so exhausted of this. Just when I think I...</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>exhausted think finally rest think maybe thing...</td>\n",
              "      <td>[9069, 2228, 2633, 2717, 2228, 2672, 2518, 270...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have been severly bullied since i was 5 till...</td>\n",
              "      <td>depression</td>\n",
              "      <td>severly bully till result depressed misanthrop...</td>\n",
              "      <td>[100, 20716, 6229, 2765, 14777, 100, 100, 3674...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text         label  \\\n",
              "0  I recently went through a breakup and she said...    depression   \n",
              "1  I do not know how to navigate these feelings, ...    depression   \n",
              "2  So I have been with my bf for 5 months , and h...    depression   \n",
              "3  I am so exhausted of this. Just when I think I...  SuicideWatch   \n",
              "4  I have been severly bullied since i was 5 till...    depression   \n",
              "\n",
              "                                           text_prep  \\\n",
              "0  recently go breakup say want friend say try ta...   \n",
              "1  know navigate feeling new feeling stretch unde...   \n",
              "2  bf month tell depressed week particular happen...   \n",
              "3  exhausted think finally rest think maybe thing...   \n",
              "4  severly bully till result depressed misanthrop...   \n",
              "\n",
              "                                            token_id  label_prep  \n",
              "0  [3728, 2175, 19010, 2360, 2215, 2767, 2360, 30...         0.0  \n",
              "1  [2113, 22149, 3110, 2047, 3110, 7683, 3305, 27...         0.0  \n",
              "2  [28939, 3204, 2425, 14777, 2733, 3327, 4148, 2...         0.0  \n",
              "3  [9069, 2228, 2633, 2717, 2228, 2672, 2518, 270...         1.0  \n",
              "4  [100, 20716, 6229, 2765, 14777, 100, 100, 3674...         0.0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HeMACk9t5T3_"
      },
      "outputs": [],
      "source": [
        "X = data['token_id']\n",
        "y = data['label_prep']\n",
        "\n",
        "\n",
        "SEED = 1235\n",
        "train_ids_0, test_ids, train_labels_0, test_labels = tts(X, y, test_size = 0.1, random_state = SEED)\n",
        "train_ids, val_ids, train_labels, val_labels = tts(train_ids_0, train_labels_0, test_size = 0.2, random_state = SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_attention_heads = int(np.random.uniform(2, 4))\n",
        "num_attention_heads = 8\n",
        "embedding_size = int(np.random.uniform(120, 170)) # ninp should be bigger\n",
        "embedding_size = embedding_size - embedding_size % num_attention_heads\n",
        "nhidden = int(np.random.uniform(50, 300))\n",
        "nhidden = 70\n",
        "nlayers = int(np.random.uniform(2, 12))\n",
        "nlayers = 4\n",
        "Dropout = np.random.uniform(0.1, 0.12)\n",
        "#criterion = nn.CrossEntropyLoss() #FIXME\n",
        "criterion = nn.BCEWithLogitsLoss() \n",
        "learning_rate = np.random.uniform(1e-3, 0.01)\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 3\n",
        "batch_size = 16\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# torch.cuda.set_device(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZXxVUBzoxzZ5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2654787/3857427028.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(padded_train, dtype=torch.float), torch.tensor(train_labels, dtype=torch.float))\n",
            "/tmp/ipykernel_2654787/3857427028.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(padded_val, dtype=torch.float), torch.tensor(val_labels_array, dtype=torch.float))\n",
            "/tmp/ipykernel_2654787/3857427028.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_dataset =TensorDataset(torch.tensor(padded_test, dtype=torch.float), torch.tensor(test_labels_array, dtype=torch.float))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#old\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# train_vectorizer = vectorizer.fit_transform(train_texts)\n",
        "# val_vectorizer   = vectorizer.transform(val_texts)\n",
        "# test_vectorizer  = vectorizer.transform(test_texts)\n",
        "\n",
        "# train_vectorizer = train_vectorizer.toarray()\n",
        "# val_vectorizer   = val_vectorizer.toarray()\n",
        "# test_vectorizer  = test_vectorizer.toarray()\n",
        "\n",
        "# _, ntoken = train_vectorizer.shape\n",
        "\n",
        "\n",
        "# train_dataset = TextDataset(train, train_labels)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# val_dataset = TensorDataset(torch.tensor(val_vectorizer, dtype=torch.float), torch.tensor(val_labels_array, dtype=torch.float))\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "# test_dataset = TensorDataset(torch.tensor(test_vectorizer, dtype=torch.float), torch.tensor(test_labels_array, dtype=torch.float))\n",
        "\n",
        "\n",
        "#new\n",
        "ntoken = len(nlp.vocab)\n",
        "ntoken= 37585\n",
        "val_labels_array = val_labels.values.astype(float)\n",
        "test_labels_array = test_labels.values.astype(float)\n",
        "\n",
        "# train_dataset = TextDataset(train_ids, train_labels)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# val_dataset = TextDataset(val_ids, val_labels)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# test_dataset = TextDataset(test_ids, test_labels) \n",
        "padded_train = pad_sequence([torch.tensor(seq) for seq in train_ids], batch_first=True, padding_value=0)\n",
        "padded_val = pad_sequence([torch.tensor(seq) for seq in val_ids], batch_first=True, padding_value=0)\n",
        "padded_test = pad_sequence([torch.tensor(seq) for seq in test_ids], batch_first=True, padding_value=0)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(padded_train, dtype=torch.float), torch.tensor(train_labels, dtype=torch.float))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(torch.tensor(padded_val, dtype=torch.float), torch.tensor(val_labels_array, dtype=torch.float))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset =TensorDataset(torch.tensor(padded_test, dtype=torch.float), torch.tensor(test_labels_array, dtype=torch.float))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "num_attention_heads= 8\n",
            "embedding_size= 144\n",
            "nhidden= 70\n",
            "nlayers= 4\n",
            "dropout= 0.10736243002159422\n",
            "criterion= BCEWithLogitsLoss()\n",
            "learning_rate= 0.003\n",
            "num_epochs= 3\n",
            "batch_size= 16\n",
            "ntoken= 37585\n"
          ]
        }
      ],
      "source": [
        "# Print the hyperparameters\n",
        "print(\"Hyperparameters:\")\n",
        "print(f\"num_attention_heads= {num_attention_heads}\")\n",
        "print(f\"embedding_size= {embedding_size}\")\n",
        "print(f\"nhidden= {nhidden}\")\n",
        "print(f\"nlayers= {nlayers}\")\n",
        "print(f\"dropout= {Dropout}\")\n",
        "print(\"criterion=\", criterion)\n",
        "print(f\"learning_rate= {learning_rate}\")\n",
        "print(f\"num_epochs= {num_epochs}\")\n",
        "print(f\"batch_size= {batch_size}\")\n",
        "print(f\"ntoken= {ntoken}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/danielochana/anaconda3/envs/conda_deep/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# model = TransformerModel(ntoken, embedding_size, num_attention_heads, nhidden, nlayers, Dropout, norm_first=True).to(device)\n",
        "model = TransformerTextClassifier( ntoken, embedding_size, num_attention_heads, nhidden, nlayers,2, Dropout, norm_first=True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ## in order to run on a trained checkpoint: \n",
        "# num_attention_heads= 8\n",
        "# embedding_size= 160\n",
        "# nhidden= 70\n",
        "# nlayers= 4\n",
        "# dropout= 0.24840944810773966\n",
        "# batch_size= 16\n",
        "# ntoken= 37585\n",
        "# learning_rate= 0.0002\n",
        "# model = TransformerTextClassifier( ntoken, embedding_size, num_attention_heads, nhidden, nlayers,2, Dropout, norm_first=True).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# checkpoint = torch.load(\"model_checkpoint3.pth\")\n",
        "# model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wRoKNCUUwuKZ",
        "outputId": "bee0f2e1-7114-4d94-bafb-f5e0cf40761c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Batch [2/916], Loss: 20.756778717041016, train_acc: 50.00%\n",
            "Epoch [1/3], Batch [32/916], Loss: 0.6332585215568542, train_acc: 52.29%\n",
            "Epoch [1/3], Batch [62/916], Loss: 0.7343567609786987, train_acc: 52.29%\n",
            "Epoch [1/3], Batch [92/916], Loss: 0.7834675312042236, train_acc: 51.88%\n",
            "Epoch [1/3], Batch [122/916], Loss: 0.649480938911438, train_acc: 51.25%\n",
            "Epoch [1/3], Batch [152/916], Loss: 1.3775031566619873, train_acc: 50.83%\n",
            "Epoch [1/3], Batch [182/916], Loss: 0.7647196054458618, train_acc: 52.71%\n",
            "Epoch [1/3], Batch [212/916], Loss: 0.579207181930542, train_acc: 53.12%\n",
            "Epoch [1/3], Batch [242/916], Loss: 0.6201026439666748, train_acc: 52.08%\n",
            "Epoch [1/3], Batch [272/916], Loss: 0.5423346757888794, train_acc: 55.21%\n",
            "Epoch [1/3], Batch [302/916], Loss: 0.629104495048523, train_acc: 57.71%\n",
            "Epoch [1/3], Batch [332/916], Loss: 0.8546398878097534, train_acc: 60.42%\n",
            "Epoch [1/3], Batch [362/916], Loss: 0.659851610660553, train_acc: 58.13%\n",
            "Epoch [1/3], Batch [392/916], Loss: 0.5569275617599487, train_acc: 59.79%\n",
            "Epoch [1/3], Batch [422/916], Loss: 0.5943460464477539, train_acc: 59.58%\n",
            "Epoch [1/3], Batch [452/916], Loss: 0.5858213901519775, train_acc: 62.08%\n",
            "Epoch [1/3], Batch [482/916], Loss: 0.73378586769104, train_acc: 60.42%\n",
            "Epoch [1/3], Batch [512/916], Loss: 0.5805546045303345, train_acc: 58.33%\n",
            "Epoch [1/3], Batch [542/916], Loss: 1.0847102403640747, train_acc: 60.42%\n",
            "Epoch [1/3], Batch [572/916], Loss: 1.769849419593811, train_acc: 58.75%\n",
            "Epoch [1/3], Batch [602/916], Loss: 0.7346962690353394, train_acc: 59.79%\n",
            "Epoch [1/3], Batch [632/916], Loss: 0.8619036078453064, train_acc: 57.92%\n",
            "Epoch [1/3], Batch [662/916], Loss: 0.7902541756629944, train_acc: 64.79%\n",
            "Epoch [1/3], Batch [692/916], Loss: 0.4191782474517822, train_acc: 70.62%\n",
            "Epoch [1/3], Batch [722/916], Loss: 1.0471885204315186, train_acc: 62.29%\n",
            "Epoch [1/3], Batch [752/916], Loss: 0.6407538652420044, train_acc: 64.17%\n",
            "Epoch [1/3], Batch [782/916], Loss: 0.7078255414962769, train_acc: 61.88%\n",
            "Epoch [1/3], Batch [812/916], Loss: 0.7738026976585388, train_acc: 62.71%\n",
            "Epoch [1/3], Batch [842/916], Loss: 0.4258650541305542, train_acc: 65.21%\n",
            "Epoch [1/3], Batch [872/916], Loss: 0.5732415318489075, train_acc: 66.04%\n",
            "Epoch [1/3], Batch [902/916], Loss: 0.47775834798812866, train_acc: 64.38%\n",
            "total_correct 2518, total_samples : 3663, Val Acc: 68.74%\n",
            "Epoch [2/3], Batch [2/916], Loss: 0.47345662117004395, train_acc: 68.75%\n",
            "Epoch [2/3], Batch [32/916], Loss: 0.5757031440734863, train_acc: 68.33%\n",
            "Epoch [2/3], Batch [62/916], Loss: 0.890823483467102, train_acc: 60.21%\n",
            "Epoch [2/3], Batch [92/916], Loss: 0.492940217256546, train_acc: 65.83%\n",
            "Epoch [2/3], Batch [122/916], Loss: 0.8020365238189697, train_acc: 67.08%\n",
            "Epoch [2/3], Batch [152/916], Loss: 0.529990553855896, train_acc: 59.79%\n",
            "Epoch [2/3], Batch [182/916], Loss: 0.44984349608421326, train_acc: 72.29%\n",
            "Epoch [2/3], Batch [212/916], Loss: 0.6115216016769409, train_acc: 69.17%\n",
            "Epoch [2/3], Batch [242/916], Loss: 0.6613479852676392, train_acc: 71.46%\n",
            "Epoch [2/3], Batch [272/916], Loss: 0.523016631603241, train_acc: 63.33%\n",
            "Epoch [2/3], Batch [302/916], Loss: 0.44294100999832153, train_acc: 67.50%\n",
            "Epoch [2/3], Batch [332/916], Loss: 0.5658043622970581, train_acc: 69.58%\n",
            "Epoch [2/3], Batch [362/916], Loss: 0.7257211208343506, train_acc: 66.46%\n",
            "Epoch [2/3], Batch [392/916], Loss: 0.4777434170246124, train_acc: 72.08%\n",
            "Epoch [2/3], Batch [422/916], Loss: 0.6748017072677612, train_acc: 69.58%\n",
            "Epoch [2/3], Batch [452/916], Loss: 0.3989941477775574, train_acc: 68.96%\n",
            "Epoch [2/3], Batch [482/916], Loss: 0.5495012998580933, train_acc: 65.83%\n",
            "Epoch [2/3], Batch [512/916], Loss: 0.6703713536262512, train_acc: 70.83%\n",
            "Epoch [2/3], Batch [542/916], Loss: 0.2701892554759979, train_acc: 67.50%\n",
            "Epoch [2/3], Batch [572/916], Loss: 0.696496844291687, train_acc: 66.25%\n",
            "Epoch [2/3], Batch [602/916], Loss: 0.814407229423523, train_acc: 70.62%\n",
            "Epoch [2/3], Batch [632/916], Loss: 0.5956392288208008, train_acc: 68.75%\n",
            "Epoch [2/3], Batch [662/916], Loss: 0.8049779534339905, train_acc: 68.12%\n",
            "Epoch [2/3], Batch [692/916], Loss: 0.6471667289733887, train_acc: 69.58%\n",
            "Epoch [2/3], Batch [722/916], Loss: 1.0017483234405518, train_acc: 60.00%\n",
            "Epoch [2/3], Batch [752/916], Loss: 0.6955245733261108, train_acc: 58.75%\n",
            "Epoch [2/3], Batch [782/916], Loss: 1.3503930568695068, train_acc: 58.96%\n",
            "Epoch [2/3], Batch [812/916], Loss: 0.42792242765426636, train_acc: 65.21%\n",
            "Epoch [2/3], Batch [842/916], Loss: 0.5134927034378052, train_acc: 66.46%\n",
            "Epoch [2/3], Batch [872/916], Loss: 0.6049959659576416, train_acc: 70.62%\n",
            "Epoch [2/3], Batch [902/916], Loss: 0.8747843503952026, train_acc: 68.75%\n",
            "total_correct 2592, total_samples : 3663, Val Acc: 70.76%\n",
            "Epoch [3/3], Batch [2/916], Loss: 0.5176306366920471, train_acc: 65.62%\n",
            "Epoch [3/3], Batch [32/916], Loss: 0.456898957490921, train_acc: 71.67%\n",
            "Epoch [3/3], Batch [62/916], Loss: 0.5352185964584351, train_acc: 76.25%\n",
            "Epoch [3/3], Batch [92/916], Loss: 0.3875853419303894, train_acc: 73.75%\n",
            "Epoch [3/3], Batch [122/916], Loss: 0.7756251096725464, train_acc: 67.08%\n",
            "Epoch [3/3], Batch [152/916], Loss: 1.0641522407531738, train_acc: 70.21%\n",
            "Epoch [3/3], Batch [182/916], Loss: 0.2934573292732239, train_acc: 66.04%\n",
            "Epoch [3/3], Batch [212/916], Loss: 0.6116303205490112, train_acc: 70.83%\n",
            "Epoch [3/3], Batch [242/916], Loss: 0.40522170066833496, train_acc: 70.21%\n",
            "Epoch [3/3], Batch [272/916], Loss: 0.314350426197052, train_acc: 72.50%\n",
            "Epoch [3/3], Batch [302/916], Loss: 0.6630653142929077, train_acc: 72.08%\n",
            "Epoch [3/3], Batch [332/916], Loss: 0.500458300113678, train_acc: 73.75%\n",
            "Epoch [3/3], Batch [362/916], Loss: 0.6435928344726562, train_acc: 72.92%\n",
            "Epoch [3/3], Batch [392/916], Loss: 1.5573947429656982, train_acc: 68.54%\n",
            "Epoch [3/3], Batch [422/916], Loss: 0.601544201374054, train_acc: 68.12%\n",
            "Epoch [3/3], Batch [452/916], Loss: 0.462581992149353, train_acc: 72.71%\n",
            "Epoch [3/3], Batch [482/916], Loss: 0.3847271203994751, train_acc: 72.92%\n",
            "Epoch [3/3], Batch [512/916], Loss: 0.7246347665786743, train_acc: 66.46%\n",
            "Epoch [3/3], Batch [542/916], Loss: 0.5478296279907227, train_acc: 69.38%\n",
            "Epoch [3/3], Batch [572/916], Loss: 0.5552825927734375, train_acc: 66.67%\n",
            "Epoch [3/3], Batch [602/916], Loss: 0.5718985795974731, train_acc: 69.79%\n",
            "Epoch [3/3], Batch [632/916], Loss: 0.5805786848068237, train_acc: 72.92%\n",
            "Epoch [3/3], Batch [662/916], Loss: 0.6832542419433594, train_acc: 73.75%\n",
            "Epoch [3/3], Batch [692/916], Loss: 0.5891338586807251, train_acc: 75.83%\n",
            "Epoch [3/3], Batch [722/916], Loss: 0.5652140974998474, train_acc: 75.00%\n",
            "Epoch [3/3], Batch [752/916], Loss: 0.7988036870956421, train_acc: 68.33%\n",
            "Epoch [3/3], Batch [782/916], Loss: 0.5371781587600708, train_acc: 68.96%\n",
            "Epoch [3/3], Batch [812/916], Loss: 0.48070117831230164, train_acc: 65.83%\n",
            "Epoch [3/3], Batch [842/916], Loss: 0.4869195818901062, train_acc: 69.79%\n",
            "Epoch [3/3], Batch [872/916], Loss: 0.4838324189186096, train_acc: 75.62%\n",
            "Epoch [3/3], Batch [902/916], Loss: 0.6508338451385498, train_acc: 66.04%\n",
            "total_correct 2591, total_samples : 3663, Val Acc: 70.73%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_correct=0.0\n",
        "    total_samples=0.0\n",
        "\n",
        "    for batch_idx, (texts, labels) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        # Convert texts and labels to tensors if necessary\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts, None)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        \n",
        "        if batch_idx % 30 == 1:\n",
        "            accuracy = total_correct / total_samples\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}, train_acc: {100. * accuracy:.2f}%\")\n",
        "            total_correct = 0.0\n",
        "            total_samples = 0.0\n",
        "\n",
        " \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for texts, labels in val_loader:\n",
        "            texts = texts.to(device).long()  \n",
        "            labels = labels.to(device)\n",
        "            outputs = model(texts, None)\n",
        "            outputs = torch.sigmoid(outputs) \n",
        "            predictions = (outputs > 0.5).float()\n",
        "            total_correct += (predictions[:,0] == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            accuracy = total_correct / total_samples\n",
        "    print(f'total_correct {total_correct}, total_samples : {total_samples}, Val Acc: {100. * accuracy:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def accurary_est(y_pred, y_label):\n",
        "  y_pred = [0 if torch.sigmoid(i) < 0.5 else 1 for i in torch.tensor(y_pred)]\n",
        "  return sum([1 for res in range(len(y_pred)) if y_pred[res] == y_label[res]]) / len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model_checkpoint7.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 11557, total_samples : 14651, Val Acc: 78.88%\n"
          ]
        }
      ],
      "source": [
        "# Final training results\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "        # print(predictions[:,0])\n",
        "        # print(f'total_correct [{predictions.shape}, total_samples : {labels.shape}')\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, Val Acc: {100. * accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 2591, total_samples : 3663, Val Acc: 70.73%\n"
          ]
        }
      ],
      "source": [
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels in val_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "        # print(predictions[:,0])\n",
        "        # print(f'total_correct [{predictions.shape}, total_samples : {labels.shape}')\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, Val Acc: {100. * accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 1450, total_samples : 2035, test Acc: 71.25%\n"
          ]
        }
      ],
      "source": [
        "# Test results\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, test Acc: {100. * accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;28;01mdel\u001b[39;00m obj  \u001b[38;5;66;03m# This removes the reference to the tensor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# This releases any remaining GPU memory not in use\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdeallocate_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mdeallocate_tensors\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m obj  \u001b[38;5;66;03m# This removes the reference to the tensor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/conda_deep/lib/python3.8/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "def deallocate_tensors():\n",
        "    for obj in gc.get_objects():\n",
        "        if torch.is_tensor(obj) :\n",
        "            if obj.device.type == 'cuda':\n",
        "                del obj  # This removes the reference to the tensor\n",
        "    torch.cuda.empty_cache()  # This releases any remaining GPU memory not in use\n",
        "\n",
        "deallocate_tensors()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.14453125\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved()\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Release cached memory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# After releasing cached memory\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/conda_deep/lib/python3.8/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_reserved()/(1024 ** 3))\n",
        "\n",
        "# Release cached memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# After releasing cached memory\n",
        "print(torch.cuda.memory_reserved()/(1024 ** 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total GPU Memory: 10.91 GB\n",
            "Allocated Memory: 0.06 GB\n",
            "Cached Memory: 0.14 GB\n",
            "Free Memory: 10.71 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def check_gpu_capacity():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        properties = torch.cuda.get_device_properties(device)\n",
        "        total_memory = properties.total_memory / (1024 ** 3)  # Convert bytes to gigabytes\n",
        "        memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)  # Allocated memory in use\n",
        "        memory_cached = torch.cuda.memory_reserved(device) / (1024 ** 3)  # Cached but not currently in use\n",
        "        free_memory = total_memory - memory_allocated - memory_cached\n",
        "        print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
        "        print(f\"Allocated Memory: {memory_allocated:.2f} GB\")\n",
        "        print(f\"Cached Memory: {memory_cached:.2f} GB\")\n",
        "        print(f\"Free Memory: {free_memory:.2f} GB\")\n",
        "    else:\n",
        "        print(\"CUDA is not available.\")\n",
        "# gc.collect()\n",
        "check_gpu_capacity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# class block(nn.Module):\n",
        "# \tdef __init__(self):\n",
        "# \t\tsuper(block, self).__init__()\n",
        "# \t\tself.attention = nn.MultiheadAttention(embeds_size, num_heads, batch_first=True)\n",
        "# \t\tself.ffn = nn.Sequential(\n",
        "# \t\t\tnn.Linear(embeds_size, 2 * embeds_size),\n",
        "# \t\t\tnn.LeakyReLU(),\n",
        "# \t\t\tnn.Linear(2 * embeds_size, embeds_size),\n",
        "# \t\t)\n",
        "# \t\tself.drop1 = nn.Dropout(drop_prob)\n",
        "# \t\tself.drop2 = nn.Dropout(drop_prob)\n",
        "# \t\tself.ln1 = nn.LayerNorm(embeds_size)\n",
        "# \t\tself.ln2 = nn.LayerNorm(embeds_size)\n",
        "\n",
        "# \tdef forward(self, hidden_state):\n",
        "# \t\tattn, _ = self.attention(hidden_state, hidden_state, hidden_state, need_weights=False)\n",
        "# \t\tattn = self.drop1(attn)\n",
        "# \t\tout = self.ln1(hidden_state + attn)\n",
        "# \t\tobserved = self.ffn(out)\n",
        "# \t\tobserved = self.drop2(observed)\n",
        "# \t\treturn self.ln2(out + observed)\n",
        "\t\n",
        "\n",
        "# class transformer(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(transformer, self).__init__()\n",
        "\n",
        "#         self.tok_emb = nn.Embedding(vocab_size, embeds_size)\n",
        "#         self.pos_emb = nn.Embedding(block_size, embeds_size)\n",
        "#         self.block = block()\n",
        "#         self.ln1 = nn.LayerNorm(embeds_size)\n",
        "#         self.ln2 = nn.LayerNorm(embeds_size)\n",
        "\n",
        "#         self.classifier_head = nn.Sequential(\n",
        "#             nn.Linear(embeds_size, embeds_size),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Dropout(drop_prob),\n",
        "#             nn.Linear(embeds_size, embeds_size),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(embeds_size, num_classes),\n",
        "#             nn.Softmax(dim=1),\n",
        "#         )\n",
        "\n",
        "#         print(\"number of parameters: %.2fM\" % (self.num_params()/1e6,))\n",
        "\n",
        "#     def num_params(self):\n",
        "#         n_params = sum(p.numel() for p in self.parameters())\n",
        "#         return n_params\n",
        "\n",
        "#     def forward(self, seq):\n",
        "#         B,T = seq.shape\n",
        "#         embedded = self.tok_emb(seq)\n",
        "#         embedded = embedded + self.pos_emb(torch.arange(T, device=device))\n",
        "#         output = self.block(embedded)\n",
        "#         output = output.mean(dim=1)\n",
        "#         output = self.classifier_head(output)\n",
        "#         return output\n",
        "    \n",
        "\n",
        "# model = transformer()\n",
        "# model.to(device)\n",
        "# vocab_size = 20000\n",
        "\n",
        "# block_size = 200\n",
        "# embeds_size = 100\n",
        "# num_classes = 2\n",
        "# drop_prob = 0.13\n",
        "# batch_size = 32\n",
        "# epochs = 30\n",
        "# num_heads = 4\n",
        "# head_size = embeds_size // num_heads\n",
        "# model_path = 'model_classification.pth'\n",
        "# model_loader = False\n",
        "\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "# \tlosses = 0\n",
        "# \tfor (inputs, targets) in train_data:\n",
        "# \t\tinputs = inputs.to(device)\n",
        "# \t\ttargets = targets.to(device)\n",
        "# \t\toutput = model(inputs)\n",
        "# \t\tloss = model_loss(output, targets)\n",
        "# \t\tmodel_optimizer.zero_grad()\n",
        "# \t\tloss.backward()\n",
        "# \t\tmodel_optimizer.step()\n",
        "# \t\tlosses += loss.item()\n",
        "# \tprint(f'[{epoch}][Train]', losses)\n",
        "# \tmodel.eval()\n",
        "# \ttest_loss = 0\n",
        "# \tpassed = 0\n",
        "# \tfor (inputs, targets) in test_data:\n",
        "# \t\twith torch.no_grad():\n",
        "# \t\t\tinputs = inputs.to(device)\n",
        "# \t\t\ttargets = targets.to(device)\n",
        "# \t\t\toutputs = model(inputs)\n",
        "# \t\t\tif outputs.argmax() == targets.argmax():\n",
        "# \t\t\t\tpassed += 1\n",
        "# \tmodel.train()\n",
        "# \tprint(f'[{epoch}][Test]', ', accuracy', passed / len(dataset_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
