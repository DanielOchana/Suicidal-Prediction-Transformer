{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Suicidal Prediction From Scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dL40voW92Y4M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import  AdamW\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import math\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import BertTokenizer\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model CLasses \n",
        "####  \n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding module for Transformer models.\n",
        "\n",
        "    Args:\n",
        "        d_model (int): The number of expected features in the input.\n",
        "        max_len (int, optional): The maximum length of the input sequence. Default is 23187.\n",
        "\n",
        "    Attributes:\n",
        "        dropout (nn.Dropout): Dropout layer.\n",
        "        pe (torch.Tensor): Positional encoding tensor.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=23187):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        pe = torch.zeros((max_len, d_model)) \n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the PositionalEncoding module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after adding positional encoding and applying dropout.\n",
        "\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerTextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based Text Classifier.\n",
        "\n",
        "    Args:\n",
        "        ntoken (int): The number of unique tokens in the input.\n",
        "        ninp (int): The number of expected features in the input.\n",
        "        nhead (int): The number of heads in the multiheadattention models.\n",
        "        nhid (int): The dimension of the feedforward network model.\n",
        "        nlayers (int): The number of sub-encoder-layers in the encoder.\n",
        "        num_classes (int): The number of classes for classification.\n",
        "        dropout (float, optional): The dropout value. Default is 0.5.\n",
        "        norm_first (bool, optional): Whether to apply layer normalization before the first encoder layer. Default is True.\n",
        "\n",
        "    Attributes:\n",
        "        pos_encoder (PositionalEncoding): PositionalEncoding module.\n",
        "        transformer_encoder (nn.TransformerEncoder): TransformerEncoder module.\n",
        "        encoder (nn.Embedding): Embedding layer.\n",
        "        ninp (int): The number of expected features in the input.\n",
        "        fc (nn.Linear): Linear layer for classification.\n",
        "        activation (nn.GELU): Activation function.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout=0.5, norm_first=True):\n",
        "        super(TransformerTextClassifier, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.fc = nn.Linear(ninp, num_classes-1)\n",
        "        self.activation = nn.GELU() \n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the TransformerTextClassifier module.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Input tensor.\n",
        "            src_mask (torch.Tensor): Mask tensor for the input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after classification.\n",
        "\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = output.mean(dim=1)  # Pooling layer (e.g., mean pooling)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        initrange = 0.1\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "                m.bias.data.zero_()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  \n",
        "---\n",
        "### Load the saved data from csv file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_string_to_array(s):\n",
        "    return np.array(ast.literal_eval(s))\n",
        "\n",
        "data['token_id']  = data['token_id'] .apply(convert_string_to_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HeMACk9t5T3_"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X = data['token_id']\n",
        "y = data['label_prep']\n",
        "SEED = 1235\n",
        "train_ids_0, test_ids, train_labels_0, test_labels = tts(X, y, test_size = 0.1, random_state = SEED)\n",
        "train_ids, val_ids, train_labels, val_labels = tts(train_ids_0, train_labels_0, test_size = 0.2, random_state = SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load saved trained model\n",
        "####  \n",
        "---\n",
        "Make sure all model hyperparams are the same as the corresponding config file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## run on a pretrained checkpoint: \n",
        "num_attention_heads= 8\n",
        "embedding_size= 160\n",
        "nhidden= 70\n",
        "nlayers= 4\n",
        "dropout= 0.24840944810773966\n",
        "batch_size= 16\n",
        "ntoken= 37585\n",
        "learning_rate= 0.0002\n",
        "model = TransformerTextClassifier( ntoken, embedding_size, num_attention_heads, nhidden, nlayers,2, Dropout, norm_first=True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "checkpoint = torch.load(\"model_checkpoint10.pth\")\n",
        "model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXxVUBzoxzZ5"
      },
      "outputs": [],
      "source": [
        "ntoken= 37585 # the size of vocabulary after tokenization\n",
        "val_labels_array = val_labels.values.astype(float)\n",
        "test_labels_array = test_labels.values.astype(float)\n",
        "\n",
        "# Pad the sequences to create a tensor of shape (batch_size, max_seq_length)\n",
        "padded_train = pad_sequence([torch.tensor(seq) for seq in train_ids], batch_first=True, padding_value=0)\n",
        "padded_val = pad_sequence([torch.tensor(seq) for seq in val_ids], batch_first=True, padding_value=0)\n",
        "padded_test = pad_sequence([torch.tensor(seq) for seq in test_ids], batch_first=True, padding_value=0)\n",
        "\n",
        "# Create DataLoader objects for the training, validation, and test sets\n",
        "train_dataset = TensorDataset(torch.tensor(padded_train, dtype=torch.float), torch.tensor(train_labels, dtype=torch.float))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(torch.tensor(padded_val, dtype=torch.float), torch.tensor(val_labels_array, dtype=torch.float))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset =TensorDataset(torch.tensor(padded_test, dtype=torch.float), torch.tensor(test_labels_array, dtype=torch.float))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate saved model\n",
        "####  \n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 11282, total_samples : 14651, Val Acc: 77.00%\n"
          ]
        }
      ],
      "source": [
        "# Final training results\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "        # print(predictions[:,0])\n",
        "        # print(f'total_correct [{predictions.shape}, total_samples : {labels.shape}')\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, Val Acc: {100. * accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 2600, total_samples : 3663, Val Acc: 70.98%\n"
          ]
        }
      ],
      "source": [
        "# Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels in val_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "        # print(predictions[:,0])\n",
        "        # print(f'total_correct [{predictions.shape}, total_samples : {labels.shape}')\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, Val Acc: {100. * accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_correct 1456, total_samples : 2035, test Acc: 71.55%\n"
          ]
        }
      ],
      "source": [
        "# Test results\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for texts, labels in test_loader:\n",
        "        texts = texts.to(device).long()  \n",
        "        labels = labels.to(device)\n",
        "        outputs = model(texts, None)\n",
        "        outputs = torch.sigmoid(outputs) \n",
        "        predictions = (outputs > 0.5).float()\n",
        "        total_correct += (predictions[:,0] == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        accuracy = total_correct / total_samples\n",
        "print(f'total_correct {total_correct}, total_samples : {total_samples}, test Acc: {100. * accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
